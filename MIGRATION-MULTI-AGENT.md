# MIGRATION-MULTI-AGENT.md â€” Plan de migration vers architecture multi-agent

> Document destinÃ© Ã  une IA de codage (Claude Code, Cursor, Clineâ€¦)
> Lire AGENTS.md et PROGRESS.md avant de commencer.
> RÃˆGLE ABSOLUE : un checkpoint validÃ© â†’ commit â†’ Ã©tape suivante.
> Ne jamais toucher Ã  un module validÃ© sans validation explicite.

---

## CONTEXTE ET OBJECTIF

### Ã‰tat actuel (avant migration)
```
main.py â€” UN SEUL CodeAgent monolithique
â”œâ”€â”€ 6 tools locaux (file_system, os_exec, clipboard, screenshot, mouse_keyboard, analyze_image)
â””â”€â”€ 26 tools MCP Chrome DevTools (via lifespan FastAPI)
= 32 tools dans un seul agent â†’ contexte surchargÃ©, modÃ¨le confus
```

### Cible aprÃ¨s migration
```
main.py â€” Manager Agent + 3 ManagedAgents spÃ©cialisÃ©s
â”œâ”€â”€ MANAGER (glm-4.7) â†’ tools directs : file_system, os_exec, clipboard
â”œâ”€â”€ pc_control_agent (qwen3-vl:2b + UI-TARS-2B) â†’ screenshot, analyze_image, mouse_keyboard
â”œâ”€â”€ browser_agent (qwen3:8b) â†’ 26 tools Chrome DevTools MCP
â””â”€â”€ web_agent (qwen3:8b) â†’ webSearchPrime, webReader, zread (TOOL-4/5/6 futurs)
```

### Pourquoi maintenant
- Avant d'ajouter TOOL-4/5/6 (MCP Z.ai) dans un agent dÃ©jÃ  surchargÃ©
- UI-TARS-2B-SFT s'intÃ¨gre naturellement dans pc_control_agent
- Chaque agent ne voit que ses tools â†’ moins de tokens, meilleure prÃ©cision
- qwen3:8b local = 0 quota Z.ai pour browser_agent et web_agent

---

## MODÃˆLES Ã€ UTILISER

| Agent | ModÃ¨le | Raison |
|-------|--------|--------|
| Manager | `glm-4.7` (reason) ou `qwen3:8b` | Orchestration, dÃ©lÃ©gation |
| pc_control_agent | `qwen3-vl:2b` (vision) | Petit, local, vision native |
| browser_agent | `qwen3:8b` (smart) | 0 quota, bon raisonnement |
| web_agent | `qwen3:8b` (smart) | 0 quota, recherche |
| UI-TARS grounding | `hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M` | Ollama local, ~1.6GB |

**Garder les modÃ¨les PLUS PETITS que les originaux :**
- `qwen3:14b` â†’ remplacÃ© par `qwen3:8b` pour les sous-agents (moins de RAM)
- `qwen3-vl:4b` â†’ remplacÃ© par `qwen3-vl:2b` dÃ©jÃ  installÃ© et validÃ©
- UI-TARS-7B â†’ UI-TARS-2B-SFT Q4_K_M (~1.6GB au lieu de ~8GB)

---

## NOUVEAU TOOL : UITarsGroundingTool (TOOL-11)

### Pourquoi UI-TARS-2B-SFT
UI-TARS-2B-SFT est un modÃ¨le ByteDance spÃ©cialisÃ© GUI grounding :
- EntraÃ®nÃ© sur des millions de screenshots avec coordonnÃ©es annotÃ©es
- Retourne des coordonnÃ©es RELATIVES [0..1] ou ABSOLUES selon le prompt
- Champion ScreenSpot benchmark (meilleur rapport taille/prÃ©cision Ã  2B)
- Tourne via Ollama avec GGUF mradermacher Q4_K_M (~1.6GB)

### Installation prÃ©alable
```bash
ollama pull hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M
```

### Format de rÃ©ponse UI-TARS
UI-TARS retourne les coordonnÃ©es au format relatif [0..1] :
```
[0.73, 0.21]
```
Il faut multiplier par la rÃ©solution Ã©cran pour obtenir les pixels absolus.

### Fichier : agent/tools/ui_tars_grounding.py

```python
"""
UITarsGroundingTool â€” DÃ©tection d'Ã©lÃ©ments UI avec UI-TARS-2B-SFT via Ollama.

SpÃ©cialisÃ© pour le GUI grounding : localise prÃ©cisÃ©ment les Ã©lÃ©ments
d'interface Ã  partir d'une description textuelle et d'un screenshot.
Retourne les coordonnÃ©es pixel absolues pour pyautogui.
"""

import logging
import os
import base64
import json
import re
from pathlib import Path
from typing import Optional

from smolagents import Tool

logger = logging.getLogger(__name__)

# Prompt systÃ¨me UI-TARS pour grounding desktop
_GROUNDING_SYSTEM = (
    "Based on the screenshot of the page, I give a text description and you give its "
    "corresponding location. The coordinate represents a clickable location [x, y] for "
    "an element, which is a relative coordinate on the screenshot, scaled from 0 to 1."
)


class UITarsGroundingTool(Tool):
    """Localise un Ã©lÃ©ment UI dans un screenshot avec UI-TARS-2B-SFT.
    
    Utilise le modÃ¨le spÃ©cialisÃ© GUI grounding UI-TARS-2B-SFT via Ollama local.
    Retourne les coordonnÃ©es pixel absolues (x, y) pour pyautogui.
    """

    name = "ui_grounding"
    description = (
        "Localise un Ã©lÃ©ment d'interface utilisateur dans un screenshot et retourne "
        "ses coordonnÃ©es pixel absolues (x, y) pour cliquer dessus avec pyautogui. "
        "Utilise UI-TARS-2B-SFT, modÃ¨le spÃ©cialisÃ© GUI grounding. "
        "Exemple: ui_grounding(image_path='C:/tmp/screen.png', element='bouton OK') "
        "â†’ retourne '{\"x\": 960, \"y\": 540, \"found\": true}'"
    )
    inputs = {
        "image_path": {
            "type": "string",
            "description": "Chemin absolu vers le screenshot PNG Ã  analyser",
        },
        "element": {
            "type": "string",
            "description": "Description textuelle de l'Ã©lÃ©ment Ã  localiser (ex: 'bouton OK', 'champ de recherche', 'menu Fichier')",
        },
    }
    output_type = "string"

    def forward(self, image_path: str, element: str) -> str:
        """
        Localise un Ã©lÃ©ment UI dans le screenshot.

        Args:
            image_path: Chemin absolu vers le screenshot
            element: Description de l'Ã©lÃ©ment Ã  localiser

        Returns:
            JSON string: {"x": int, "y": int, "found": bool, "rel_x": float, "rel_y": float}
            ou "ERROR: ..." en cas d'Ã©chec
        """
        import requests
        from PIL import Image

        try:
            # VÃ©rifier que le fichier existe
            if not Path(image_path).exists():
                return f"ERROR: Screenshot non trouvÃ©: {image_path}"

            # Obtenir les dimensions de l'image pour conversion coordonnÃ©es relatives â†’ absolues
            with Image.open(image_path) as img:
                screen_width, screen_height = img.size

            logger.info(f"UI-TARS grounding: '{element}' dans {image_path} ({screen_width}x{screen_height})")

            # Encoder l'image en base64
            with open(image_path, "rb") as f:
                image_b64 = base64.b64encode(f.read()).decode("utf-8")

            # Appel Ollama avec UI-TARS-2B-SFT
            ollama_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
            response = requests.post(
                f"{ollama_url}/api/chat",
                json={
                    "model": "hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M",
                    "messages": [
                        {
                            "role": "user",
                            "content": f"{_GROUNDING_SYSTEM}\n\n{element}",
                            "images": [image_b64],
                        }
                    ],
                    "stream": False,
                    "options": {
                        "temperature": 0.0,  # DÃ©terministe pour le grounding
                        "num_ctx": 4096,
                    },
                },
                timeout=60,  # UI-TARS-2B est rapide
            )
            response.raise_for_status()

            raw_output = response.json().get("message", {}).get("content", "").strip()
            logger.info(f"UI-TARS output brut: {raw_output}")

            # Parser les coordonnÃ©es relatives [x, y] retournÃ©es par UI-TARS
            coords = self._parse_coordinates(raw_output)
            if coords is None:
                return json.dumps({
                    "found": False,
                    "error": f"Impossible de parser les coordonnÃ©es depuis: {raw_output}",
                    "raw": raw_output,
                })

            rel_x, rel_y = coords

            # Convertir en coordonnÃ©es absolues pixel
            abs_x = int(rel_x * screen_width)
            abs_y = int(rel_y * screen_height)

            logger.info(f"Ã‰lÃ©ment '{element}' trouvÃ©: rel=({rel_x:.3f}, {rel_y:.3f}) â†’ abs=({abs_x}, {abs_y})")

            return json.dumps({
                "found": True,
                "x": abs_x,
                "y": abs_y,
                "rel_x": round(rel_x, 4),
                "rel_y": round(rel_y, 4),
                "screen_size": f"{screen_width}x{screen_height}",
                "element": element,
            })

        except requests.Timeout:
            return "ERROR: Timeout UI-TARS (>60s) â€” modÃ¨le peut-Ãªtre non chargÃ©"
        except requests.RequestException as e:
            return f"ERROR: Ollama non accessible: {e}"
        except Exception as e:
            logger.error(f"Erreur UITarsGroundingTool: {e}", exc_info=True)
            return f"ERROR: {type(e).__name__}: {e}"

    def _parse_coordinates(self, text: str) -> Optional[tuple[float, float]]:
        """Parse les coordonnÃ©es relatives [x, y] depuis la rÃ©ponse UI-TARS."""
        # UI-TARS retourne typiquement: [0.73, 0.21]
        # Parfois avec du texte autour
        patterns = [
            r'\[(\d+\.?\d*),\s*(\d+\.?\d*)\]',   # [0.73, 0.21]
            r'\((\d+\.?\d*),\s*(\d+\.?\d*)\)',   # (0.73, 0.21)
            r'(\d+\.?\d*),\s*(\d+\.?\d*)',         # 0.73, 0.21
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                x, y = float(match.group(1)), float(match.group(2))
                # Valider que les coordonnÃ©es sont dans [0, 1]
                if 0 <= x <= 1 and 0 <= y <= 1:
                    return x, y
        return None
```

---

## Ã‰TAPE 1 â€” CrÃ©er UITarsGroundingTool (TOOL-11)

**Fichier Ã  crÃ©er** : `agent/tools/ui_tars_grounding.py`
**Contenu** : voir section NOUVEAU TOOL ci-dessus

**Modifications `agent/tools/__init__.py`** :
```python
from .ui_tars_grounding import UITarsGroundingTool

TOOLS = [
    FileSystemTool(),
    OsExecTool(),
    ClipboardTool(),
    ScreenshotTool(),
    MouseKeyboardTool(),
    VisionTool(),
    UITarsGroundingTool(),  # â† AJOUTER ICI
]
```

**DÃ©pendances** : Pillow dÃ©jÃ  prÃ©sente, requests dÃ©jÃ  prÃ©sente. Rien Ã  ajouter.

**PrÃ©requis Ollama** :
```bash
ollama pull hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M
```

### Checkpoint Ã‰TAPE 1
Test dans Gradio avec modÃ¨le `smart` (qwen3:8b) :
1. "Prends un screenshot, puis localise le bouton DÃ©marrer de Windows"
   â†’ Attendu : JSON avec coordonnÃ©es pixel proches du coin bas-gauche
2. "Prends un screenshot du bureau, localise l'icÃ´ne de la corbeille"
   â†’ Attendu : JSON {"found": true, "x": ..., "y": ..., "rel_x": ..., "rel_y": ...}
3. VÃ©rifier dans les logs Ollama que `UI-TARS-2B-SFT-GGUF` est bien appelÃ©

**Commit** : `feat(tools): tool-11 ui-tars grounding 2b`

---

## Ã‰TAPE 2 â€” CrÃ©er les sous-agents spÃ©cialisÃ©s

### 2A â€” CrÃ©er `agent/agents/pc_control_agent.py`

```python
"""
pc_control_agent â€” Agent spÃ©cialisÃ© pilotage PC Windows.

Outils : screenshot, analyze_image (qwen3-vl:2b), ui_grounding (UI-TARS-2B), mouse_keyboard
ModÃ¨le : qwen3-vl:2b (vision native, 100% local)
RÃ´le : Voir l'Ã©cran, localiser les Ã©lÃ©ments, cliquer, taper
"""

import os
import logging
from smolagents import CodeAgent, LiteLLMModel, ManagedAgent

logger = logging.getLogger(__name__)

# Instructions spÃ©cifiques pc_control_agent
_PC_CONTROL_INSTRUCTIONS = """
Tu es un agent spÃ©cialisÃ© dans le pilotage de l'interface graphique Windows.

WORKFLOW OBLIGATOIRE pour toute action :
1. screenshot() â†’ capture l'Ã©tat actuel de l'Ã©cran
2. analyze_image(image_path=..., prompt="DÃ©cris ce que tu vois") â†’ comprendre l'Ã©cran
3. ui_grounding(image_path=..., element="description de l'Ã©lÃ©ment") â†’ obtenir coordonnÃ©es {x, y}
4. mouse_keyboard(operation="click", x=..., y=...) â†’ cliquer avec les coordonnÃ©es absolues

RÃˆGLES IMPORTANTES :
- TOUJOURS prendre un screenshot avant d'agir
- TOUJOURS utiliser ui_grounding pour obtenir les coordonnÃ©es â€” NE JAMAIS inventer des coordonnÃ©es
- AprÃ¨s chaque action importante, reprendre un screenshot pour vÃ©rifier
- Si ui_grounding retourne {"found": false}, essayer une description diffÃ©rente de l'Ã©lÃ©ment
- Pour taper du texte : d'abord cliquer sur le champ, puis mouse_keyboard(operation="type", text="...")
- Pour ouvrir une app : mouse_keyboard(operation="hotkey", keys="win"), puis type le nom, puis Enter

EXEMPLES DE GROUNDING :
- ui_grounding(image_path=screenshot_path, element="bouton OK")
- ui_grounding(image_path=screenshot_path, element="champ de recherche Windows")  
- ui_grounding(image_path=screenshot_path, element="barre des tÃ¢ches bouton DÃ©marrer")
- ui_grounding(image_path=screenshot_path, element="zone de texte Notepad")
"""


def create_pc_control_agent(ollama_url: str) -> tuple[CodeAgent, ManagedAgent]:
    """
    CrÃ©e le sous-agent de pilotage PC avec vision + UI-TARS grounding.
    
    Returns:
        Tuple (agent, managed_agent) pour utilisation dans le manager
    """
    from tools import TOOLS

    # Filtrer uniquement les tools pertinents pour le pilotage PC
    pc_tools_names = {"screenshot", "analyze_image", "ui_grounding", "mouse_keyboard"}
    pc_tools = [t for t in TOOLS if t.name in pc_tools_names]

    if not pc_tools:
        raise RuntimeError(f"Aucun outil PC trouvÃ©. Outils disponibles: {[t.name for t in TOOLS]}")

    logger.info(f"pc_control_agent tools: {[t.name for t in pc_tools]}")

    # ModÃ¨le : qwen3-vl:2b (vision native, 100% local, dÃ©jÃ  installÃ© et validÃ©)
    model = LiteLLMModel(
        model_id="ollama_chat/qwen3-vl:2b",
        api_base=ollama_url,
        api_key="ollama",
        num_ctx=8192,          # Plus de contexte pour screenshots encodÃ©s en base64
        extra_body={"think": False},
    )

    agent = CodeAgent(
        tools=pc_tools,
        model=model,
        max_steps=15,           # Plus d'Ã©tapes car workflow screenshotâ†’visionâ†’groundingâ†’action
        verbosity_level=1,
        additional_authorized_imports=["json", "re", "time", "os"],
        executor_kwargs={"timeout_seconds": 300},
        instructions=_PC_CONTROL_INSTRUCTIONS,
    )

    managed = ManagedAgent(
        agent=agent,
        name="pc_control",
        description=(
            "Agent spÃ©cialisÃ© pour piloter l'interface graphique Windows. "
            "Peut voir l'Ã©cran (screenshot), comprendre l'interface (vision IA), "
            "localiser prÃ©cisÃ©ment les Ã©lÃ©ments UI (UI-TARS grounding), "
            "et interagir avec la souris et le clavier. "
            "Utilise-le pour : ouvrir des applications, cliquer sur des boutons, "
            "remplir des formulaires, naviguer dans Windows."
        ),
    )

    return agent, managed
```

### 2B â€” CrÃ©er `agent/agents/browser_agent.py`

```python
"""
browser_agent â€” Agent spÃ©cialisÃ© pilotage Chrome via DevTools MCP.

Outils : 26 tools Chrome DevTools MCP (navigation, click, fill, screenshot, snapshot...)
ModÃ¨le : qwen3:8b (local, 0 quota)
RÃ´le : Naviguer sur le web, remplir des formulaires, extraire du contenu
"""

import os
import logging
from contextlib import contextmanager
from smolagents import CodeAgent, LiteLLMModel, ManagedAgent, ToolCollection
from mcp import StdioServerParameters

logger = logging.getLogger(__name__)

_BROWSER_INSTRUCTIONS = """
Tu es un agent spÃ©cialisÃ© dans l'automatisation de Chrome via Chrome DevTools MCP.

WORKFLOW RECOMMANDÃ‰ :
1. navigate_page(url=...) â†’ naviguer vers une URL
2. take_snapshot() â†’ obtenir la structure de la page avec les uid des Ã©lÃ©ments
3. click(uid=...) ou fill(uid=..., value=...) â†’ interagir avec les Ã©lÃ©ments
4. wait_for(text=...) â†’ attendre le chargement si nÃ©cessaire

BONNES PRATIQUES :
- Toujours take_snapshot() avant d'interagir pour connaÃ®tre les uid
- PrÃ©fÃ©rer take_snapshot() Ã  take_screenshot() (plus rapide, uid exploitables)
- Utiliser wait_for() aprÃ¨s une navigation si la page charge lentement
- Pour les recherches web : Ã©viter Google (CAPTCHA), prÃ©fÃ©rer DuckDuckGo ou Bing
- Utiliser evaluate_script() pour extraire du contenu dynamique
- Toujours retourner un rÃ©sumÃ© clair de ce qui a Ã©tÃ© fait ou trouvÃ©
"""


def create_browser_managed_agent(
    ollama_url: str,
    mcp_tools: list,
) -> ManagedAgent:
    """
    CrÃ©e le sous-agent browser avec les tools Chrome DevTools MCP dÃ©jÃ  chargÃ©s.
    
    Args:
        ollama_url: URL du serveur Ollama
        mcp_tools: Liste des tools MCP dÃ©jÃ  initialisÃ©s (depuis lifespan)
    
    Returns:
        ManagedAgent wrappant le browser agent
    """
    if not mcp_tools:
        logger.warning("browser_agent: aucun tool MCP Chrome DevTools disponible")

    # ModÃ¨le : qwen3:8b local (0 quota, bon pour navigation structurÃ©e)
    model = LiteLLMModel(
        model_id="ollama_chat/qwen3:8b",
        api_base=ollama_url,
        api_key="ollama",
        num_ctx=32768,
        extra_body={"think": False},
    )

    agent = CodeAgent(
        tools=mcp_tools,
        model=model,
        max_steps=12,
        verbosity_level=1,
        additional_authorized_imports=["json", "re", "time"],
        executor_kwargs={"timeout_seconds": 240},
        instructions=_BROWSER_INSTRUCTIONS,
    )

    managed = ManagedAgent(
        agent=agent,
        name="browser",
        description=(
            "Agent spÃ©cialisÃ© dans l'automatisation de Chrome. "
            "Peut naviguer vers des URLs, prendre des snapshots de pages web, "
            "cliquer sur des Ã©lÃ©ments, remplir des formulaires, exÃ©cuter du JavaScript, "
            "et extraire du contenu de pages web. "
            "Utilise-le pour : visiter des sites, faire des recherches web, "
            "remplir des formulaires en ligne, extraire des donnÃ©es de pages web."
        ),
    )

    return managed
```

### 2C â€” CrÃ©er `agent/agents/web_agent.py`

```python
"""
web_agent â€” Agent spÃ©cialisÃ© recherche et lecture web via MCP Z.ai.

Outils : webSearchPrime, webReader, zread (chargÃ©s dynamiquement si ZAI_API_KEY)
ModÃ¨le : qwen3:8b (local, 0 quota pour le LLM)
RÃ´le : Recherche web temps rÃ©el, lecture d'articles, exploration de repos GitHub
"""

import logging
from smolagents import CodeAgent, LiteLLMModel, ManagedAgent

logger = logging.getLogger(__name__)

_WEB_INSTRUCTIONS = """
Tu es un agent spÃ©cialisÃ© dans la recherche et la lecture de contenu web.

OUTILS DISPONIBLES (si configurÃ©s) :
- webSearchPrime(search_query="...", search_recency_filter="oneWeek") â†’ recherche web temps rÃ©el
- webReader(url="...") â†’ lire le contenu complet d'une page web
- search_doc / get_repo_structure / read_file â†’ explorer des repos GitHub publics

BONNES PRATIQUES :
- Garder les requÃªtes de recherche courtes et prÃ©cises (max 70 caractÃ¨res)
- Utiliser search_recency_filter="oneWeek" pour les actualitÃ©s rÃ©centes
- Utiliser search_domain_filter="huggingface.co" pour cibler un site prÃ©cis
- RÃ©sumer les rÃ©sultats de maniÃ¨re concise et structurÃ©e
- Si aucun tool Z.ai n'est disponible (pas de ZAI_API_KEY), le signaler clairement

QUOTA : 100 calls/mois partagÃ©s entre recherche, lecture et GitHub. Utiliser avec parcimonie.
"""


def create_web_managed_agent(
    ollama_url: str,
    web_search_tools: list,
) -> ManagedAgent | None:
    """
    CrÃ©e le sous-agent web avec les tools MCP Z.ai dÃ©jÃ  chargÃ©s.
    Retourne None si aucun tool web n'est disponible.
    
    Args:
        ollama_url: URL du serveur Ollama
        web_search_tools: Liste des tools MCP Z.ai (peut Ãªtre vide)
    
    Returns:
        ManagedAgent ou None si pas de tools disponibles
    """
    if not web_search_tools:
        logger.warning("web_agent: aucun tool web MCP disponible (ZAI_API_KEY manquant?)")
        return None

    model = LiteLLMModel(
        model_id="ollama_chat/qwen3:8b",
        api_base=ollama_url,
        api_key="ollama",
        num_ctx=32768,
        extra_body={"think": False},
    )

    agent = CodeAgent(
        tools=web_search_tools,
        model=model,
        max_steps=8,
        verbosity_level=1,
        additional_authorized_imports=["json", "re"],
        executor_kwargs={"timeout_seconds": 120},
        instructions=_WEB_INSTRUCTIONS,
    )

    managed = ManagedAgent(
        agent=agent,
        name="web_search",
        description=(
            "Agent spÃ©cialisÃ© dans la recherche web et la lecture de contenu en ligne. "
            "Peut effectuer des recherches web en temps rÃ©el, lire des articles et pages web, "
            "et explorer des repositories GitHub publics. "
            "Utilise-le pour : trouver des informations rÃ©centes, lire la documentation, "
            "explorer du code source sur GitHub."
        ),
    )

    return managed
```

### 2D â€” CrÃ©er `agent/agents/__init__.py`

```python
"""Agents package â€” sous-agents spÃ©cialisÃ©s pour my-claw."""
```

### Checkpoint Ã‰TAPE 2
VÃ©rifier que les fichiers sont crÃ©Ã©s sans erreur d'import :
```bash
cd agent
uv run python -c "from agents.pc_control_agent import create_pc_control_agent; print('OK')"
uv run python -c "from agents.browser_agent import create_browser_managed_agent; print('OK')"
uv run python -c "from agents.web_agent import create_web_managed_agent; print('OK')"
```

**Commit** : `refactor(agents): structure multi-agent â€” crÃ©er sous-agents spÃ©cialisÃ©s`

---

## Ã‰TAPE 3 â€” Refondre main.py

C'est la modification centrale. Remplacer le CodeAgent monolithique par le Manager + sous-agents.

### main.py complet (remplacement total)

```python
import os
import logging
import re
import requests
from pathlib import Path
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from smolagents import CodeAgent, LiteLLMModel, ManagedAgent, ToolCollection
from mcp import StdioServerParameters
from dotenv import load_dotenv
from tools import TOOLS

load_dotenv()

# â”€â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)
logging.getLogger("smolagents").setLevel(logging.DEBUG)
logging.getLogger("LiteLLM").setLevel(logging.INFO)


# â”€â”€â”€ Skills â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_skills() -> str:
    skills_path = Path(__file__).parent / "skills.txt"
    try:
        with open(skills_path, "r", encoding="utf-8") as f:
            skills = f.read()
        logger.info(f"âœ“ Skills chargÃ©s ({len(skills)} chars)")
        return skills
    except FileNotFoundError:
        logger.warning("âœ— skills.txt non trouvÃ©")
        return "You are a Python coding expert. Always use final_answer() to return results."
    except Exception as e:
        logger.error(f"âœ— Erreur chargement skills: {e}")
        return ""

SKILLS = load_skills()


# â”€â”€â”€ DÃ©tection modÃ¨les Ollama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PREFERENCES: dict[str, list[str]] = {
    "fast":   ["gemma3:latest", "qwen3:4b", "gemma3n:latest"],
    "smart":  ["qwen3:8b", "qwen3:4b", "gemma3n:latest", "gemma3:latest"],
    "main":   ["qwen3:8b", "qwen3:4b", "gemma3n:latest", "gemma3:latest"],
    "vision": ["qwen3-vl:2b", "qwen3-vl:4b", "llama3.2-vision"],
}

CLOUD_MODELS: dict[str, tuple[str, str]] = {
    "code":   ("openai/glm-4.7-flash", os.environ.get("ZAI_BASE_URL", "https://api.z.ai/api/coding/paas/v4")),
    "reason": ("openai/glm-4.7",       os.environ.get("ZAI_BASE_URL", "https://api.z.ai/api/coding/paas/v4")),
}

_detected_models: dict[str, tuple[str, str]] | None = None


def get_ollama_models() -> list[str]:
    try:
        ollama_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
        response = requests.get(f"{ollama_url}/api/tags", timeout=5)
        response.raise_for_status()
        return [m["name"] for m in response.json().get("models", [])]
    except Exception as e:
        logger.warning(f"Ollama non accessible: {e}")
        return []


def detect_models() -> dict[str, tuple[str, str]]:
    global _detected_models
    if _detected_models is not None:
        return _detected_models

    ollama_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    available = get_ollama_models()
    logger.info(f"ModÃ¨les Ollama dÃ©tectÃ©s: {available}")

    detected = {}
    for category, preferences in MODEL_PREFERENCES.items():
        for model_name in preferences:
            if model_name in available:
                detected[category] = (f"ollama_chat/{model_name}", ollama_url)
                logger.info(f"âœ“ {category}: {model_name}")
                break
        else:
            logger.warning(f"âœ— {category}: aucun modÃ¨le trouvÃ© parmi {preferences}")

    # VÃ©rifier prÃ©sence UI-TARS pour pc_control_agent
    uitars_model = "hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M"
    if uitars_model in available:
        logger.info(f"âœ“ UI-TARS-2B-SFT dÃ©tectÃ© pour pc_control_agent")
    else:
        logger.warning(f"âœ— UI-TARS-2B-SFT non trouvÃ© â€” installer avec: ollama pull {uitars_model}")

    detected.update(CLOUD_MODELS)
    _detected_models = detected
    return detected


MODELS = detect_models()


# â”€â”€â”€ GLM-4.7 cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_glm_response(text: str) -> str:
    """Nettoie les balises </code parasites gÃ©nÃ©rÃ©es par GLM-4.7."""
    if not text:
        return text
    text = re.sub(r'</code>?\s*(\n|$)', r'\1', text)
    text = re.sub(r'</s>\s*(\n|$)', r'\1', text)
    text = re.sub(r'</code>\s*$', '', text)
    text = re.sub(r'</code\s*$', '', text)
    text = re.sub(r'</s>\s*$', '', text)
    return text


class CleanedLiteLLMModel(LiteLLMModel):
    def generate(self, messages, stop_sequences=None, response_format=None,
                 tools_to_call_from=None, **kwargs):
        chat_message = super().generate(messages, stop_sequences, response_format,
                                        tools_to_call_from, **kwargs)
        if chat_message.content:
            original_len = len(chat_message.content)
            chat_message.content = clean_glm_response(chat_message.content)
            if original_len != len(chat_message.content):
                logger.info(f"âœ“ GLM cleanup: {original_len} â†’ {len(chat_message.content)} chars")
        return chat_message


def get_model(model_id: str = "main") -> LiteLLMModel:
    if model_id not in MODELS:
        if "main" in MODELS:
            model_name, base_url = MODELS["main"]
        elif MODELS:
            model_name, base_url = next(iter(MODELS.values()))
            logger.warning(f"ModÃ¨le '{model_id}' non trouvÃ©, fallback")
        else:
            raise RuntimeError("Aucun modÃ¨le disponible.")
    else:
        model_name, base_url = MODELS[model_id]

    is_glm = "z.ai" in base_url.lower() or model_id in ["code", "reason"]

    if is_glm:
        return CleanedLiteLLMModel(
            model_id=model_name,
            api_base=base_url,
            api_key=os.environ.get("ZAI_API_KEY", "ollama"),
            stop=["</code>", "</code", "</s>"],
        )
    else:
        return LiteLLMModel(
            model_id=model_name,
            api_base=base_url,
            api_key="ollama",
            num_ctx=32768,
            extra_body={"think": False},
        )


# â”€â”€â”€ MCP Chrome DevTools â€” Ã©tat global lifespan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_chrome_mcp_context: ToolCollection | None = None
_chrome_mcp_tools: list = []

# MCP Z.ai (web search, web reader, zread) â€” chargÃ©s dans lifespan
_web_search_context: ToolCollection | None = None
_web_search_tools: list = []


@asynccontextmanager
async def lifespan(app: FastAPI):
    global _chrome_mcp_context, _chrome_mcp_tools
    global _web_search_context, _web_search_tools

    # â”€â”€ Chrome DevTools MCP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("Initialisation Chrome DevTools MCP...")
    try:
        chrome_params = StdioServerParameters(
            command="npx",
            args=["-y", "chrome-devtools-mcp@latest"],
            env={**os.environ},
        )
        _chrome_mcp_context = ToolCollection.from_mcp(chrome_params, trust_remote_code=True)
        tool_collection = _chrome_mcp_context.__enter__()
        _chrome_mcp_tools = list(tool_collection.tools)
        logger.info(f"âœ“ Chrome DevTools MCP: {len(_chrome_mcp_tools)} outils")
    except Exception as e:
        logger.warning(f"âœ— Chrome DevTools MCP: {e}")
        _chrome_mcp_context = None
        _chrome_mcp_tools = []

    # â”€â”€ Web Search MCP Z.ai (TOOL-4) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # IMPORTANT : dÃ©commenter quand ZAI_API_KEY configurÃ© et TOOL-4 implÃ©mentÃ©
    # logger.info("Initialisation Web Search MCP Z.ai...")
    # try:
    #     if os.environ.get("ZAI_API_KEY"):
    #         web_search_params = {
    #             "url": "https://api.z.ai/api/mcp/web_search_prime/mcp",
    #             "type": "streamable-http",
    #             "headers": {"Authorization": f"Bearer {os.environ['ZAI_API_KEY']}"}
    #         }
    #         _web_search_context = ToolCollection.from_mcp(web_search_params, trust_remote_code=True)
    #         tool_collection = _web_search_context.__enter__()
    #         _web_search_tools = list(tool_collection.tools)
    #         logger.info(f"âœ“ Web Search MCP Z.ai: {len(_web_search_tools)} outils")
    #     else:
    #         logger.warning("âœ— ZAI_API_KEY absent, Web Search MCP dÃ©sactivÃ©")
    # except Exception as e:
    #     logger.warning(f"âœ— Web Search MCP Z.ai: {e}")

    yield

    # â”€â”€ Shutdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if _chrome_mcp_context is not None:
        try:
            _chrome_mcp_context.__exit__(None, None, None)
            logger.info("âœ“ Chrome DevTools MCP fermÃ©")
        except Exception as e:
            logger.error(f"âœ— Fermeture Chrome MCP: {e}")

    if _web_search_context is not None:
        try:
            _web_search_context.__exit__(None, None, None)
            logger.info("âœ“ Web Search MCP Z.ai fermÃ©")
        except Exception as e:
            logger.error(f"âœ— Fermeture Web Search MCP: {e}")


app = FastAPI(title="my-claw agent", version="0.2.0", lifespan=lifespan)


# â”€â”€â”€ Tools directs du Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Le manager utilise seulement les tools simples (fichiers, OS, clipboard)
# Les tools vision/screenshot/mouse sont dans pc_control_agent
MANAGER_TOOLS_NAMES = {"file_system", "os_exec", "clipboard"}

def get_manager_tools() -> list:
    """Tools directs du manager (fichiers, OS, clipboard uniquement)."""
    return [t for t in TOOLS if t.name in MANAGER_TOOLS_NAMES]


# â”€â”€â”€ Construction du systÃ¨me multi-agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_multi_agent_system(model_id: str = "main") -> CodeAgent:
    """
    Construit le systÃ¨me Manager + sous-agents selon les tools disponibles.
    
    Architecture :
    - Manager : glm-4.7 ou qwen3:8b + tools directs (file_system, os_exec, clipboard)
    - pc_control : qwen3-vl:2b + screenshot, analyze_image, ui_grounding, mouse_keyboard
    - browser : qwen3:8b + Chrome DevTools MCP (si disponible)
    - web_search : qwen3:8b + MCP Z.ai (si ZAI_API_KEY configurÃ©)
    """
    from agents.pc_control_agent import create_pc_control_agent
    from agents.browser_agent import create_browser_managed_agent
    from agents.web_agent import create_web_managed_agent

    ollama_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    managed_agents = []

    # â”€â”€ Sous-agent pilotage PC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        _, managed_pc = create_pc_control_agent(ollama_url)
        managed_agents.append(managed_pc)
        logger.info("âœ“ pc_control_agent crÃ©Ã© (screenshot + vision + UI-TARS + mouse/keyboard)")
    except Exception as e:
        logger.warning(f"âœ— pc_control_agent non disponible: {e}")

    # â”€â”€ Sous-agent browser Chrome â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if _chrome_mcp_tools:
        try:
            managed_browser = create_browser_managed_agent(ollama_url, _chrome_mcp_tools)
            managed_agents.append(managed_browser)
            logger.info(f"âœ“ browser_agent crÃ©Ã© ({len(_chrome_mcp_tools)} tools Chrome DevTools)")
        except Exception as e:
            logger.warning(f"âœ— browser_agent non disponible: {e}")
    else:
        logger.warning("âœ— browser_agent ignorÃ© (Chrome DevTools MCP non disponible)")

    # â”€â”€ Sous-agent web search Z.ai â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if _web_search_tools:
        try:
            managed_web = create_web_managed_agent(ollama_url, _web_search_tools)
            if managed_web:
                managed_agents.append(managed_web)
                logger.info(f"âœ“ web_agent crÃ©Ã© ({len(_web_search_tools)} tools Z.ai)")
        except Exception as e:
            logger.warning(f"âœ— web_agent non disponible: {e}")
    else:
        logger.info("âœ— web_agent ignorÃ© (aucun tool MCP Z.ai)")

    # â”€â”€ Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    manager_tools = get_manager_tools()
    logger.info(f"Manager tools directs: {[t.name for t in manager_tools]}")
    logger.info(f"Sous-agents disponibles: {[m.name for m in managed_agents]}")

    manager = CodeAgent(
        tools=manager_tools,
        model=get_model(model_id),
        managed_agents=managed_agents,
        max_steps=10,
        verbosity_level=2,
        additional_authorized_imports=[
            "requests", "urllib", "json", "csv", "pathlib", "os", "subprocess",
        ],
        executor_kwargs={"timeout_seconds": 240},
        instructions=SKILLS,
    )

    return manager


# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt_with_history(message: str, history: list[dict]) -> str:
    if not history:
        return message
    lines = [
        f"{'User' if m['role'] == 'user' else 'Assistant'}: {m['content']}"
        for m in history[-10:]
    ]
    return f"Previous conversation:\n{chr(10).join(lines)}\n\nCurrent message: {message}"


# â”€â”€â”€ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RunRequest(BaseModel):
    message: str
    history: list[dict] = []
    model: str = "main"


@app.post("/run")
async def run(req: RunRequest):
    try:
        agent = build_multi_agent_system(req.model)
        prompt = build_prompt_with_history(req.message, req.history)
        result = agent.run(prompt)
        return {"response": str(result)}
    except Exception as e:
        logger.error(f"Agent error: {type(e).__name__}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health():
    return {
        "status": "ok",
        "module": "2-multi-agent",
        "chrome_mcp": len(_chrome_mcp_tools),
        "web_mcp": len(_web_search_tools),
    }


@app.get("/models")
async def list_models():
    models_info = {}
    for category, (model_name, base_url) in MODELS.items():
        display_name = model_name.split("/")[-1] if "/" in model_name else model_name
        is_local = "ollama_chat/" in model_name or "localhost" in base_url
        models_info[category] = {
            "name": display_name,
            "full_name": model_name,
            "type": "local" if is_local else "cloud",
            "available": True,
        }
    return {
        "models": models_info,
        "ollama_models": get_ollama_models(),
        "sub_agents": {
            "pc_control": "qwen3-vl:2b + UI-TARS-2B-SFT",
            "browser": f"qwen3:8b + {len(_chrome_mcp_tools)} tools Chrome DevTools",
            "web_search": f"qwen3:8b + {len(_web_search_tools)} tools Z.ai MCP",
        },
    }
```

### Checkpoint Ã‰TAPE 3
```bash
cd agent
uv run uvicorn main:app --reload
```
VÃ©rifier dans les logs :
```
âœ“ Chrome DevTools MCP: 26 outils
âœ“ pc_control_agent crÃ©Ã© (screenshot + vision + UI-TARS + mouse/keyboard)
âœ“ browser_agent crÃ©Ã© (26 tools Chrome DevTools)
```

Test basique via curl :
```bash
curl -X POST http://localhost:8000/run \
  -H "Content-Type: application/json" \
  -d '{"message": "Liste les fichiers dans C:/tmp", "model": "smart"}'
```

**Commit** : `refactor(main): migration architecture multi-agent manager + sous-agents`

---

## Ã‰TAPE 4 â€” Mettre Ã  jour gradio_app.py (Gradio 6.6.0)

### Changements Gradio 6 vs 5
- `gr.ChatInterface` : paramÃ¨tre `type="messages"` devient obligatoire en Gradio 6
- Historique : toujours format `list[dict]` avec `role`/`content`
- Nouveau : `gr.ChatInterface` accepte `additional_inputs_accordion` pour mieux organiser

### gradio_app.py mis Ã  jour (Gradio 6.6.0)

```python
"""
Gradio 6.6.0 â€” Interface de dÃ©veloppement my-claw multi-agent.
Compatible Gradio 6.x (type="messages" obligatoire).
"""

import gradio as gr
import requests
import os
from dotenv import load_dotenv

load_dotenv()

AGENT_URL = os.environ.get("AGENT_URL", "http://localhost:8000")


def get_available_models() -> list[str]:
    """RÃ©cupÃ¨re les modÃ¨les disponibles depuis l'agent."""
    try:
        resp = requests.get(f"{AGENT_URL}/models", timeout=5)
        resp.raise_for_status()
        data = resp.json()
        models = list(data.get("models", {}).keys())
        return models if models else ["fast", "smart", "main", "vision", "code", "reason"]
    except Exception:
        return ["fast", "smart", "main", "vision", "code", "reason"]


def chat(
    message: str,
    history: list[dict],  # Gradio 6 : toujours list[dict] avec type="messages"
    model_choice: str,
) -> str:
    """
    Fonction de chat compatible Gradio 6.6.0.
    history est dÃ©jÃ  au format list[dict] avec type="messages".
    """
    # Convertir l'historique Gradio 6 au format attendu par l'API
    history_dicts = []
    for m in history:
        if isinstance(m, dict) and "role" in m and "content" in m:
            history_dicts.append({"role": m["role"], "content": str(m["content"])})

    try:
        resp = requests.post(
            f"{AGENT_URL}/run",
            json={"message": message, "history": history_dicts, "model": model_choice},
            timeout=360,  # 6 minutes pour les tÃ¢ches complexes multi-agent
        )
        resp.raise_for_status()
        return resp.json()["response"]
    except requests.Timeout:
        return "â±ï¸ Timeout (6min) â€” tÃ¢che trop longue ou modÃ¨le surchargÃ©."
    except requests.ConnectionError:
        return "âŒ Agent non accessible sur http://localhost:8000 â€” dÃ©marrer l'agent d'abord."
    except Exception as e:
        return f"âŒ Erreur: {e}"


def get_agent_status() -> str:
    """VÃ©rifie le statut de l'agent et des sous-agents."""
    try:
        resp = requests.get(f"{AGENT_URL}/health", timeout=3)
        data = resp.json()
        chrome = data.get("chrome_mcp", 0)
        web = data.get("web_mcp", 0)
        return (
            f"âœ… Agent en ligne | "
            f"Chrome DevTools: {chrome} tools | "
            f"Web MCP: {web} tools"
        )
    except Exception:
        return "âŒ Agent hors ligne â€” dÃ©marrer: `uv run uvicorn main:app --reload`"


# â”€â”€ Interface Gradio 6.6.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AVAILABLE_MODELS = get_available_models()

with gr.Blocks(title="my-claw â€” Dev Interface") as demo:
    gr.Markdown("# ðŸ¦ž my-claw â€” Interface de dÃ©veloppement")
    gr.Markdown("Architecture multi-agent : Manager â†’ pc_control | browser | web_search")

    with gr.Row():
        status_box = gr.Textbox(
            label="Statut agent",
            value=get_agent_status(),
            interactive=False,
            scale=4,
        )
        refresh_btn = gr.Button("ðŸ”„ RafraÃ®chir", scale=1)

    refresh_btn.click(fn=get_agent_status, outputs=status_box)

    with gr.Row():
        model_dropdown = gr.Dropdown(
            choices=AVAILABLE_MODELS,
            value="smart" if "smart" in AVAILABLE_MODELS else AVAILABLE_MODELS[0],
            label="ModÃ¨le Manager",
            info="Le manager dÃ©lÃ¨gue aux sous-agents selon la tÃ¢che",
            scale=2,
        )

    # ChatInterface Gradio 6.6.0 â€” type="messages" obligatoire
    chat_interface = gr.ChatInterface(
        fn=chat,
        type="messages",          # â† OBLIGATOIRE en Gradio 6
        additional_inputs=[model_dropdown],
        examples=[
            ["Liste les fichiers dans C:/tmp"],
            ["Prends un screenshot et dÃ©cris ce que tu vois"],
            ["Ouvre Chrome sur https://example.com et prends un snapshot"],
            ["Ouvre Notepad et tape 'Bonjour depuis my-claw !'"],
            ["Prends un screenshot, localise le bouton DÃ©marrer, et clique dessus"],
        ],
        title=None,               # Titre gÃ©rÃ© par le Blocks parent
    )


if __name__ == "__main__":
    demo.launch(server_port=7860, share=False)
```

### Checkpoint Ã‰TAPE 4
```bash
cd agent
uv run python gradio_app.py
```
- Ouvrir http://localhost:7860
- VÃ©rifier que le statut agent s'affiche correctement
- Tester un message simple ("Liste les fichiers dans C:/tmp")
- VÃ©rifier que l'historique fonctionne sur plusieurs tours

**Commit** : `feat(gradio): migration gradio 6.6.0 + interface multi-agent`

---

## Ã‰TAPE 5 â€” Mettre Ã  jour pyproject.toml

```toml
[project]
name = "my-claw-agent"
version = "0.2.0"
description = "my-claw â€” Agent Python smolagents multi-agent"
requires-python = ">=3.11"

dependencies = [
    "smolagents[litellm,mcp]>=1.9.0",
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "pydantic>=2.9.0",
    "requests>=2.32.0",
    "httpx>=0.27.0",
    "gradio>=6.6.0",          # â† Mis Ã  jour depuis 5.x
    "python-dotenv>=1.0.0",
    "pyperclip>=1.11.0",
    "pyautogui>=0.9.54",
    "pillow>=12.1.1",
    "mcp>=0.9.0",
    "mcpadapt>=0.1.19",
]

[project.optional-dependencies]
dev = [
    "ruff>=0.8.0",
    "pyright>=1.1.0",
]
```

Mise Ã  jour dÃ©pendances :
```bash
cd agent
uv add "gradio>=6.6.0"
uv sync
```

**Commit** : `chore(deps): gradio 6.6.0 + version 0.2.0`

---

## Ã‰TAPE 6 â€” Tests de validation end-to-end

### 6A â€” Test Manager seul (tools directs)
Prompt : "CrÃ©e le fichier C:/tmp/migration_test.txt avec le contenu 'Multi-agent OK'"
Attendu : Le manager utilise file_system directement, sans dÃ©lÃ©guer

### 6B â€” Test dÃ©lÃ©gation pc_control
Prompt : "Prends un screenshot de l'Ã©cran et dÃ©cris ce que tu vois"
Attendu dans les logs :
```
Manager â†’ dÃ©lÃ¨gue Ã  pc_control_agent
pc_control_agent â†’ screenshot() â†’ analyze_image() â†’ final_answer(description)
```

### 6C â€” Test dÃ©lÃ©gation browser
Prompt : "Ouvre https://example.com dans Chrome et donne-moi le titre de la page"
Attendu :
```
Manager â†’ dÃ©lÃ¨gue Ã  browser_agent
browser_agent â†’ navigate_page() â†’ take_snapshot() â†’ final_answer(titre)
```

### 6D â€” Test pilotage PC avec UI-TARS
Prompt : "Prends un screenshot, trouve le bouton DÃ©marrer Windows et donne ses coordonnÃ©es"
Attendu :
```
pc_control_agent â†’ screenshot() â†’ ui_grounding(element="bouton DÃ©marrer") 
â†’ {"found": true, "x": 15, "y": 1065, ...}
```

### 6E â€” Test tÃ¢che complÃ¨te multi-agent
Prompt : "Ouvre Notepad via le menu DÃ©marrer et tape 'Test migration multi-agent OK'"
Attendu : pc_control_agent orchestre screenshot â†’ ui_grounding â†’ mouse_keyboard en sÃ©quence

**Commit** : `test(multi-agent): validation end-to-end tous sous-agents`

---

## Ã‰TAPE 7 â€” Mettre Ã  jour PROGRESS.md et LEARNING.md

### Sections Ã  mettre Ã  jour dans PROGRESS.md

Remplacer la section "MODULE TOOLS" par :

```markdown
## ARCHITECTURE MULTI-AGENT â€” DONE

Migration vers architecture Manager + 3 sous-agents spÃ©cialisÃ©s (2026-02-21)

Manager (glm-4.7 / qwen3:8b) â†’ tools directs : file_system, os_exec, clipboard
â”œâ”€â”€ pc_control_agent (qwen3-vl:2b) â†’ screenshot, analyze_image, ui_grounding, mouse_keyboard
â”œâ”€â”€ browser_agent (qwen3:8b) â†’ 26 tools Chrome DevTools MCP
â””â”€â”€ web_agent (qwen3:8b) â†’ webSearchPrime, webReader, zread (activer avec ZAI_API_KEY)

## TOOL-11 â€” UITarsGroundingTool
**Statut : DONE**
- ModÃ¨le : hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M (~1.6GB)
- Retourne coordonnÃ©es absolues pixel depuis description textuelle + screenshot
- IntÃ©grÃ© dans pc_control_agent
```

### Section Ã  ajouter dans LEARNING.md

```markdown
## Architecture Multi-Agent â€” Migration (2026-02-21)

### DÃ©cisions architecture
- ManagedAgent smolagents : wrapping agent â†’ callable comme tool par le manager
- pc_control_agent utilise qwen3-vl:2b (vision native) au lieu de qwen3:8b
- browser_agent : qwen3:8b suffit (pas besoin de vision, snapshot = texte)
- web_agent : crÃ©Ã© vide si pas de ZAI_API_KEY, retourne None proprement

### UI-TARS-2B-SFT coordonnÃ©es
- Retourne [rel_x, rel_y] dans [0..1]
- Conversion : abs_x = int(rel_x * screen_width)
- temperature=0.0 obligatoire pour grounding dÃ©terministe
- ModÃ¨le : hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M via Ollama

### Gradio 6.6.0 breaking changes
- type="messages" obligatoire dans gr.ChatInterface (Gradio 6)
- history format : list[dict] avec "role" et "content" (toujours en Gradio 6)
- gr.Blocks + gr.ChatInterface ensemble : title=None dans ChatInterface
```

**Commit** : `docs: mise Ã  jour PROGRESS.md et LEARNING.md migration multi-agent`

---

## RÃ‰CAPITULATIF ORDRE D'IMPLÃ‰MENTATION

```
Ã‰TAPE 1  UITarsGroundingTool (TOOL-11)         â† CrÃ©er tools/ui_tars_grounding.py
Ã‰TAPE 2  Sous-agents spÃ©cialisÃ©s               â† CrÃ©er agents/ package (3 fichiers)
Ã‰TAPE 3  Refonte main.py                        â† Manager + ManagedAgents
Ã‰TAPE 4  Mise Ã  jour gradio_app.py              â† Gradio 6.6.0
Ã‰TAPE 5  Mise Ã  jour pyproject.toml             â† gradio>=6.6.0
Ã‰TAPE 6  Tests end-to-end (6Aâ†’6E)              â† Valider chaque dÃ©lÃ©gation
Ã‰TAPE 7  Mise Ã  jour PROGRESS.md + LEARNING.md â† Documentation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ CHECKPOINT FINAL validÃ© â†’ passer TOOL-4 (MCP Web Search Z.ai)
```

---

## STRUCTURE REPO APRÃˆS MIGRATION

```
agent/
â”œâ”€â”€ main.py                    â† Manager + lifespan MCP + endpoints
â”œâ”€â”€ gradio_app.py              â† Gradio 6.6.0
â”œâ”€â”€ skills.txt                 â† Skills partagÃ©s (manager + sous-agents)
â”œâ”€â”€ pyproject.toml             â† gradio>=6.6.0, version 0.2.0
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pc_control_agent.py    â† qwen3-vl:2b + screenshot/vision/ui_tars/mouse
â”‚   â”œâ”€â”€ browser_agent.py       â† qwen3:8b + Chrome DevTools MCP
â”‚   â””â”€â”€ web_agent.py           â† qwen3:8b + MCP Z.ai (TOOL-4/5/6)
â””â”€â”€ tools/
    â”œâ”€â”€ __init__.py            â† TOOLS = [7 outils dont UITarsGroundingTool]
    â”œâ”€â”€ file_system.py
    â”œâ”€â”€ os_exec.py
    â”œâ”€â”€ clipboard.py
    â”œâ”€â”€ vision.py
    â”œâ”€â”€ screenshot.py
    â”œâ”€â”€ mouse_keyboard.py
    â””â”€â”€ ui_tars_grounding.py   â† NOUVEAU TOOL-11
```

---

## PRÃ‰REQUIS AVANT DE COMMENCER

```bash
# 1. Installer UI-TARS-2B-SFT via Ollama
ollama pull hf.co/mradermacher/UI-TARS-2B-SFT-GGUF:Q4_K_M

# 2. VÃ©rifier que qwen3-vl:2b est bien installÃ©
ollama list | grep qwen3-vl

# 3. VÃ©rifier que qwen3:8b est installÃ© (pour browser_agent et web_agent)
ollama list | grep qwen3:8b

# 4. Mettre Ã  jour gradio
cd agent && uv add "gradio>=6.6.0"
```

---

## VARIABLES ENV â€” AUCUN CHANGEMENT REQUIS

Les variables existantes dans agent/.env suffisent :
```env
OLLAMA_BASE_URL=http://localhost:11434
ZAI_API_KEY=ton_token_zai          # Optionnel â€” active web_agent si prÃ©sent
ZAI_BASE_URL=https://api.z.ai/api/coding/paas/v4
SCREENSHOT_DIR=C:\tmp\myclawshots
```

---

## NOTES IMPORTANTES POUR L'IA DE CODAGE

1. **Ordre strict** : respecter Ã‰TAPE 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7
2. **Un checkpoint validÃ© avant de passer Ã  l'Ã©tape suivante**
3. **Ne pas modifier** TOOL-1, TOOL-2, TOOL-3, TOOL-7, TOOL-8 â€” ils sont validÃ©s
4. **TOOL-9 (mouse_keyboard)** reste en cours â€” intÃ©grÃ© dans pc_control_agent mais validation end-to-end Ã  faire en Ã‰TAPE 6E
5. **web_agent** crÃ©e un ManagedAgent `None` si pas de tools â€” gÃ©rer ce cas dans build_multi_agent_system
6. **GLM-4.7 stop sequences** : maintenir `CleanedLiteLLMModel` pour les modÃ¨les cloud
7. **Gradio 6.6.0** : `type="messages"` est obligatoire â€” sinon TypeError au runtime
8. **UI-TARS coordonnÃ©es** : toujours vÃ©rifier que rel_x et rel_y sont dans [0..1] avant conversion â€” rejeter si hors bornes
9. **qwen3:8b** : vÃ©rifier qu'il est bien installÃ© avant de crÃ©er browser_agent et web_agent
