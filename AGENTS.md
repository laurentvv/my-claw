# AGENTS.md ‚Äî my-claw Assistant Personnel Hybride
> Fichier de guidage pour les IA de codage (Kilo Code, Crush, Claude Code, Cursor, Codex, Windsurf...)
> Architecture : Next.js 16 (gateway) + Python smolagents (cerveau) + Gradio (UI dev)

---

## R√àGLES IMP√âRATIVES POUR L'IA DE CODAGE

0. **Lis LEARNING.md** - avant chaque t√¢che. Mets **LEARNING.md** √† jour avec les nouvelles d√©couvertes apr√®s.
1. **STOP √† chaque CHECKPOINT** ‚Äî attendre validation explicite de l'utilisateur avant de continuer
2. **Un module √† la fois** ‚Äî ne jamais anticiper le module suivant
3. **Lire le skill correspondant** dans `.claude/skills/` avant de coder quoi que ce soit
4. **Ne jamais impl√©menter un module V2** ‚Äî ils sont marqu√©s en attente et bloqu√©s intentionnellement
5. **uv uniquement** pour Python ‚Äî jamais pip install, jamais requirements.txt
6. **Pas de console.log** en prod c√¥t√© Next.js ‚Äî logger structur√© uniquement
7. **Pas de secrets dans le code** ‚Äî toujours process.env ou os.environ
8. **Webhooks** : r√©pondre HTTP 200 imm√©diatement, traiter en async
9. **Valider TypeScript** : npx tsc --noEmit doit passer avant chaque commit
10. **lancement serveur** : Ne jamais exectuter : **npm run** dev ou **uv run uvicorn main:app --reload**, demander √† l'utilisateur de le lancer et de donn√© les erreurs/informations
11. **max_steps** : 5 pour t√¢ches simples, 10 pour pilotage PC complexe (TOOL-9)

---

## STACK TECHNIQUE

| Couche | Technologie | Version | Notes |
|--------|-------------|---------|-------|
| Gateway | Next.js | 16+ | App Router obligatoire |
| ORM | Prisma | 7+ | SQLite local |
| Runtime JS | Node.js | 24+ | |
| Gestionnaire Python | uv | latest | Jamais pip |
| Agent LLM | smolagents | 1.9+ | CodeAgent |
| API serveur Python | FastAPI | 0.115+ | |
| UI dev | Gradio | 5+ | |
| LLM local | Ollama | latest | Port 11434 |
| LLM cloud | Z.ai GLM-4.7 | ‚Äî | OpenAI-compatible |
| Language | TypeScript | 5+ | strict: true |

---

## MOD√àLES LLM

### Ollama ‚Äî 100% local, 0 donn√©e sortante

| ID | Mod√®le | Taille | Usage |
|----|--------|--------|-------|
| fast | gemma3:latest | 3.3GB | R√©ponses rapides |
| smart | qwen3:8b | 5.2GB | Usage quotidien ‚Äî recommand√© |
| main | qwen3:8b | 5.2GB | Mod√®le principal ‚Äî d√©faut |
| vision | qwen3-vl:2b | 2.3GB | Vision locale (TOOL-7) |

### Z.ai ‚Äî Cloud (donn√©es envoy√©es √† Z.ai) - OPTIONNEL

| ID | Mod√®le | Usage |
|----|--------|-------|
| code | glm-4.7-flash | Code, t√¢ches techniques |
| reason | glm-4.7 | Raisonnement profond |

R√®gles mod√®les :
- Mod√®le par d√©faut : main (qwen3:8b)
- **D√©tection automatique** : L'agent d√©tecte les mod√®les Ollama install√©s au d√©marrage via `GET /api/tags`
- **Pr√©f√©rences par cat√©gorie** : Chaque cat√©gorie (fast/smart/main/vision) a une liste de mod√®les pr√©f√©r√©s
- **Fallback intelligent** : Si le mod√®le pr√©f√©r√© n'est pas install√©, utilise le suivant dans la liste
- Si ZAI_API_KEY absent : fallback silencieux sur main
- think: false en mode agent (√©vite verbosit√© Qwen3)
- num_ctx: 32768 pour tous les mod√®les Ollama
- max_steps=5 pour t√¢ches simples, 10 pour pilotage PC complexe
- Provider Ollama : LiteLLMModel avec prefix ollama_chat/
- Provider Z.ai : LiteLLMModel avec prefix openai/ (compatible OpenAI)
- TOOL-7 (analyze_image) : utilise qwen3-vl:2b en local via Ollama
- **API `/models`** : Endpoint pour r√©cup√©rer la liste des mod√®les disponibles

---

## SCH√âMA BASE DE DONN√âES

### Prisma 7 ‚Äî Points critiques
- url dans schema.prisma n'existe plus
- La connexion se configure dans gateway/prisma.config.ts via datasource.url
- adapter, earlyAccess, engine sont supprim√©s en v7
- PrismaLibSQL prend { url: string } directement, pas besoin de createClient()

### Table Conversation
- id : cuid, PK
- channel : "webchat" | "nextcloud"
- channelId : identifiant unique par canal (session, num√©ro, username)
- title : r√©sum√© auto, optionnel
- model : mod√®le par d√©faut "main"
- createdAt, updatedAt
- Index unique sur [channel, channelId]

### Table Message
- id : cuid, PK
- conversationId : FK vers Conversation (cascade delete)
- role : "user" | "assistant"
- content : texte
- model : mod√®le utilis√©, optionnel
- createdAt
- Index sur [conversationId, createdAt]

### Table CronJob
- id : cuid, PK
- name : unique
- schedule : expression cron ex "0 9 * * 1-5"
- prompt : instruction √† ex√©cuter
- channel + channelId : destinataire
- model : d√©faut "main"
- enabled : boolean
- lastRun : datetime optionnel

### Table Settings
- key : PK string
- value : string
- updatedAt
- Utilis√© pour : system_prompt, persona_name, persona_lang

---

## CANAUX

### WebChat
- Auth : header Authorization: Bearer {WEBCHAT_TOKEN}
- Streaming via Server-Sent Events (ReadableStream)
- Token minimum 32 chars

### Nextcloud Talk Bot
- POST /api/webhook/nextcloud : v√©rifier signature HMAC-SHA256 obligatoire
- Headers √† v√©rifier : X-Nextcloud-Talk-Random + X-Nextcloud-Talk-Signature
- Secret partag√© : NEXTCLOUD_BOT_SECRET
- Envoi : POST {NC_BASE_URL}/ocs/v2.php/apps/spreed/api/v1/bot/{BOT_ID}/message
- Header envoi : OCS-APIRequest: true
- Utiliser crypto.timingSafeEqual pour comparer les signatures

---

## FLUX /api/chat

1. Recevoir { message, conversationId?, model?, channel, channelId }
2. Cr√©er ou r√©cup√©rer la Conversation (Prisma)
3. Sauvegarder le message user en DB
4. R√©cup√©rer les 20 derniers messages (contexte glissant)
5. R√©cup√©rer le system prompt depuis Settings
6. Appeler agent Python POST /run avec { message, history, model }
7. Sauvegarder la r√©ponse assistant en DB
8. Retourner la r√©ponse (stream SSE pour webchat, JSON pour les autres)

---

## OUTILS smolagents

### V1 ‚Äî Actifs (impl√©ment√©s et valid√©s)
- **FileSystemTool** (TOOL-1) : read/write/create/delete/list/move/search
- **OsExecTool** (TOOL-2) : ex√©cution PowerShell
- **ClipboardTool** (TOOL-3) : lecture/√©criture presse-papier
- **ScreenshotTool** (TOOL-8) : capture d'√©cran Windows
- **ChromeDevTools MCP** (TOOL-10) : pilotage Chrome (Puppeteer)

### V1 ‚Äî En cours (non valid√©)
- **MouseKeyboardTool** (TOOL-9) : üîÑ contr√¥le souris/clavier (n√©cessite orchestration)

### V1 ‚Äî Roadmap (√Ä venir)
- **Web Search MCP** (TOOL-4) : ‚è≥ recherche web Z.ai
- **Web Reader MCP** (TOOL-5) : ‚è≥ lecture URL Z.ai
- **Zread MCP** (TOOL-6) : ‚è≥ lecture GitHub Z.ai

### V2 ‚Äî Bloqu√©s, ne pas impl√©menter
- run_code sandbox Python/Node
- read_file / write_file dossier whitelist

### R√®gles de codage outils
- Toujours sous-classe Tool (pas d√©corateur @tool) pour compatibilit√© Ollama
- Imports dans forward(), pas au top-level du fichier
- Timeout 10s sur tous les appels HTTP
- max_steps=5 pour t√¢ches simples, 10 pour pilotage PC complexe
- pyautogui.FAILSAFE=False configur√© (pas de coin haut-gauche pour arr√™ter)
- time.sleep(0.5) apr√®s chaque action pyautogui pour laisser l'OS r√©agir
- Logs de debug ajout√©s dans mouse_keyboard.py pour diagnostiquer les probl√®mes

---

## VARIABLES D'ENVIRONNEMENT

### gateway/.env.local
- DATABASE_URL : file:./prisma/dev.db
- WEBCHAT_TOKEN : min 32 chars (openssl rand -hex 32)
- CRON_SECRET : min 32 chars
- AGENT_URL : http://localhost:8000
- NEXTCLOUD_BASE_URL
- NEXTCLOUD_BOT_SECRET
- NEXTCLOUD_BOT_ID
- SCREENSHOT_DIR : C:\tmp\myclawshots (optionnel, d√©faut: C:\tmp\myclawshots)

### agent/.env
- OLLAMA_BASE_URL : http://localhost:11434
- ZAI_API_KEY : optionnel
- ZAI_BASE_URL : https://api.z.ai/api/coding/paas/v4
- SCREENSHOT_DIR : C:\tmp\myclawshots (d√©faut)

---

## CONVENTIONS DE CODE

### Python
- Python 3.11+, type hints partout
- pyproject.toml + uv.lock ‚Äî jamais requirements.txt
- uv add <pkg> pour ajouter une d√©pendance
- uv run <cmd> pour ex√©cuter
- Logging via module logging, pas print()

### TypeScript
- strict: true dans tsconfig
- Exports nomm√©s, pas de default sauf pages Next.js
- unknown + type guard, jamais any

### S√©curit√©
- Ne jamais logger le contenu des messages
- Logger uniquement : canal, timestamp, dur√©e, mod√®le
- Valider signature/token sur tous les webhooks
- pyautogui.FAILSAFE=False configur√© (pas de coin haut-gauche pour arr√™ter)
- time.sleep(0.5) apr√®s chaque action pyautogui pour laisser l'OS r√©agir

---

## CE QU'ON NE FAIT PAS

- Pas de multi-utilisateurs
- Pas de Docker pour l'app principale
- Pas de Redis ou message queue
- Pas de pip install ou requirements.txt
- Pas de features V2 sans validation explicite
- Pas de Telegram, Discord, Slack, Signal
- Pas de pilotage PC sans Vision (TOOL-7 requis pour TOOL-9)

---

## CHECKLIST AVANT COMMIT

- npx tsc --noEmit passe sans erreur (gateway)
- Les deux services d√©marrent sans erreur
- Nouvelles variables d'env dans .env.example
- Pas de secrets dans le code
- Webhooks v√©rifient leur signature/token
- CHECKPOINT du module valid√© par l'utilisateur

---

## R√âF√âRENCES

- smolagents : https://huggingface.co/docs/smolagents
- smolagents MCP : https://huggingface.co/docs/smolagents/tutorials/mcp
- Prisma 7 Config : https://pris.ly/d/config-datasource
- Z.ai GLM-4.7 : https://open.bigmodel.cn/dev/api
- Ollama API : https://github.com/ollama/ollama/blob/main/docs/api.md
- Nextcloud Talk Bot : https://nextcloud-talk.readthedocs.io/en/latest/bot-list/
- Prisma 7 Docs : https://www.prisma.io/docs
- Next.js 16 : https://nextjs.org/docs/app
- Gradio : https://www.gradio.app/docs
- FastAPI : https://fastapi.tiangolo.com
- pyautogui : https://pyautogui.readthedocs.io
- Pillow : https://pillow.readthedocs.io
